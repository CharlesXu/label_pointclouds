\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{algorithmicx}
\author{Jessica Chemali, Kumar Shaurya Shankar, Trenton Tabor\\ \{jessb,kumarsha\}@cs.cmu.edu, ttabor@nrec.ri.cmu.edu}
\title{16831 RoboStats Lab 2: \\Online Learning}
\begin{document}
\maketitle
\section{Overview}
\subsection{Discussion about Dataset}
The single biggest issue in the dataset is the bias in the data points. For instance the ground class and the vegetation class have two orders of magnitude data than, say the wires class. This disparity in the number of data points adversely affects linear classifier performance specially in a one-vs-all regime that we implement in this lab assignment. Thus, as shown later in the results, some classes are easily distinguishable but others like wire/pole are not. To mitigate this somewhat, we have tried to duplicate under-represented data in the training data. Further, since some of our algorithms are sensitive to the order in which training data is presented, we also shuffle the training data. We implemented k-fold cross validation to tune our parameters.
%----------------------------------------------------------------------
\section{Implemented methods}
\subsection{Bayes Linear Regression}
\subsubsection{Implementation}
The Bayes Linear Regressor has been implemented in Python, as is the rest of the framework. The external library dependency is only on numpy for matrix operations and normal distribution functions. We use scikit-learn for calculating the confusion matrix. The algorithm has not been modified for the purpose of this implementation.
\subsubsection{Parameter Selection}
The three parameter choices are the initial natural parameters for the prior on the weight vector, and the noise variance for the observation error. The latter was arbitrarily chosen based on manual parameter tweaking. For the prior, a 0 mean weight vector was chosen, and a hyper parameter of 1.0 for the precision matrix is chosen.

It was observed that because of the disparity in the class data for one vs all, and since mere duplication of under-represented data is not a good reflection of the actual distribution, we use margin scaling to assign a label of +10 to positive samples, and -1 to negative samples. This works fairly well in practice.
\subsubsection{Performance}
Please see fig \ref{BLR_perf} and table \ref{BLR_tab}
\begin{figure}[h]
\centering
\includegraphics[width=0.4\linewidth]{figs/BLR_3D_amtoan.png}
\includegraphics[width=0.4\linewidth]{figs/blr_amtoan.png}
\caption{Bayes Linear Regression trained on the am dataset, tested on the an dataset. The confusion matrix has been scaled column-wise to enhance visibility.}
\vspace{10pt}
\includegraphics[width=0.4\linewidth, trim = 0 0 0 35, clip]{figs/BLR_3D_antoam.png}
\includegraphics[width=0.4\linewidth]{figs/blr_antoam.png}
\caption{Bayes Linear Regression trained on the am dataset, tested on the an dataset. }
\label{BLR_perf}
\end{figure}
\begin{table}[h]
\centering
\begin{tabular}{|c|c|c||c|c|c|c|c||c||}
\hline
Comment&$\sigma$&$\Sigma_{prior}$&Veg & Wire & Pole & Ground & Facade & Accuracy\\ \hline
No noise&0.2 & 1.0 & 0.86951241 & 0.31436837 & 0.44126984 & 0.9800258 &  0.72926606 & 0.8518\\ \hline
+ 100 Random & 0.2 & 1.0 & 0.88313143 & 0.06222577 & 0.47450485 & 0.97512645 & 0.7285905  & 0.8557\\ \hline
$0.1 \sigma$ noise & 0.2 & 1.0 & 0.66242779 & 0.0436386  & 0.32891247 & 0.95459632 & 0.55975769 & 0.7186 \\ \hline
\end{tabular}
\caption{F1 score for BLR trained on am and tested on an}
\vspace{10pt}
\begin{tabular}{||c|c|c||c|c|c|c|c||c||}
\hline
Comment&$\sigma$&$\Sigma_{prior}$&Veg & Wire & Pole & Ground & Facade & Accuracy\\ \hline
No noise & 0.2 & 1.0& 0.87475848 & 0.35217954  &0.74221157  &0.99572913  &0.86710144 & 0.9554\\ \hline
+ 100 Random & 0.2 & 1.0 & 0.83830351 & 0.38423645  &0.68479117  &0.9956858  &0.86833179 &0.9538 \\ \hline
$0.1 \sigma$ noise & 0.2 & 1.0 & 0.583568 & 0.20260936 & 0.49171271 & 0.99528651  &0.45985532 & 0.8772\\ \hline
\end{tabular}
\caption{F1 score for BLR trained on an and tested on am}
\label{BLR_tab}
\end{table}
\subsubsection{Learning Duration}
For a typical, noise free run
Training duration =  2.15s\\
Testing duration =  1.11s

The complexity of the training is $O(n*d^2*c)$, since we see $n$ observations, compute the precision matrix with a $d\times d$ dimension and train $c$ classifiers. The complexity of testing is $O(d^3*c)$ as we need to invert the precision matrix to obtain the mean of the weight vector distribution, and have to choose from $c$ classes.
\subsubsection{Robustness to Noise}
To test robustness we first added 100 uniform features to the feature vector. This resulted in a surprisingly negligible drop in performance.  When adding $0.1\sigma$ gaussian noise to the feature vector, however, the performance degraded significantly (see table)
%------------------------------------------------------------------------
\subsection{Multiclass exponentiated gradient algorithm}

\subsubsection{Winnow with binary features}
We first implemented Winnow algorithm with binary features, after discretizing every feature based on the resulting entropy of the partition. It gave terrible results, because it turned out that the binary features weren't informative enough to differentiate well between the classes and the performance of the algorithm depended tremendously on the order the observations were presented to the algorithm. 

\subsubsection{Generalization of winnow to continuous features}

The algorithm that we will discuss is similar to winnow but instead, when the algorithm makes a mistake, it updates the weight of feature $i$ using the equation : $w_i = w_i*exp(\nu x_i y_i)$. 

Training online was affected considerably by the order the data was presented to the algorithm. This is why we shuffled the data before training the algorithm and in this way reached better results.

Training on the "an" dataset and testing on the held out "am" dataset was easier than the other way around and resulted in an accuracy of around $90\%$ versus an accuracy of $70\%$ on the held out datasets. The confusion matrix, which is more useful showed that the algorithm predicted very well class $1200$ with very little negatives, followed by class $1004$, and class $1400$. Classes $1100$ and $1103$, corresponding to the poles and wires were poorly learned. This is not surprising because the datasets were highly unbalanced and relatively very few observations were from a pole or a wire. Although we tried to balance the number of observations from each class, the configuration of the features observed were still limited.

The implementation of the algorithm is straightforward, it is only a few lines of code in python. We train binary learners for each of the class and combine them for multi-class classification. The algorithm is very fast. For learning it loops through the $n$ observations and at each step it updates the weights of all $d$ features. We learn $c$ classifiers. The computational complexity of learning is therefore $O(n*d*c)$. For prediction, a dot product is taken for each of the classes and the best one is selected, so the computational complexity is $O(d*c)$

For this algorithm we had 2 hyperparameters to tune: the learning rate $\nu$, and a threshold $\theta$. We looked at $\nu = \{0.001,0.01,0.1\} \times \theta = \{5,10,15\}$ and compared them against each other using 5-fold cross validation. the results were obtained with $\nu = 0.01$ and $\theta = 10$.

\subsubsection{Performance}
Please see fig \ref{winnow_perf} and table \ref{winnow_tab}. Typical runtime is 
Training duration =  0.7940s
Testing duration =  1.0329s
\begin{figure}[h]
\centering
\includegraphics[width=0.4\linewidth]{figs/winnow_3D_amtoan.png}
\includegraphics[width=0.4\linewidth]{figs/winnow_am_to_an.png}
\caption{Multi Class Winnow trained on the am dataset, tested on the an dataset. The confusion matrix has been scaled column-wise to enhance visibility.}
\vspace{10pt}
\includegraphics[width=0.4\linewidth, trim = 0 0 0 35, clip]{figs/winnow_3D_antoam.png}
\includegraphics[width=0.4\linewidth]{figs/winnow_an_to_am.png}
\caption{Multi Class Winnow trained on the an dataset, tested on the am dataset. }
\label{winnow_perf}
\end{figure}

\begin{table}[h]
\centering
\begin{tabular}{||c|c|c||c|c|c|c|c||c||}
\hline
Comment&$\theta$&$\nu$&Veg & Wire & Pole & Ground & Facade & Accuracy\\ \hline
No Noise&10 & 0.1 &  0.69866904 & 0.00249169  &0.19875776 & 0.98499241 & 0.4491265&0.7380\\ \hline
+100 Random&10 & 0.1 &  0.68435176 & 0.00913242&  0.52292535 & 0.98608336 & 0.53118099 &0.7467\\ \hline
$0.1 \sigma$&10 & 0.1 & 0.62216288 & 0.   &       0.38092345  &0.97497053 & 0.50215229 & 0.7077\\ \hline
\end{tabular}
\caption{F1 score for Winnow trained on am and tested on an}
\vspace{10pt}
\begin{tabular}{||c|c|c||c|c|c|c|c||c||}
\hline
Comment&$\theta$&$\nu$&Veg & Wire & Pole & Ground & Facade & Accuracy\\ \hline
No Noise&10 & 0.1 &  0.61175467&  0.2037779  & 0.48552064  &0.99495698 & 0.73060609&0.8968\\ \hline
+100 Random&10 & 0.1 &  0.5594218   &0.19851852 & 0.45119522 & 0.99630558 & 0.2908644 & 0.8614\\ \hline
$0.1 \sigma$&10 & 0.1 & 0.56291784 & 0.23488045  &0.53571429 & 0.99622961 & 0.36735472 & 0.8677\\ \hline
\end{tabular}
\caption{F1 score for Winnow trained on an and tested on am}
\label{winnow_tab}
\end{table}
%----------------------------------------------------------------------
\subsection{Kernalized SVM}
\subsubsection{Implementation}
We use the algorithm discussed in class for implementation. Specific modifications that we make are to not compute the regularizer term of the loss function for performance reasons. We do this since estimating the $\langle f, f \rangle = \alpha^T K_{ij} \alpha$ is an expensive operation. The update step, however, uses the standard regularizer term for shrinking $\alpha$s at each step. The kernel we choose is a simple dot product kernel, i.e. $k(x_i, x_j) = x_i^T x_j$. In order for the support vectors to not fill up memory and slow down the training, we choose to drop support vectors for which the magnitude of $\alpha$ falls below $1e-10$

For each trained one vs all predictor we add positive samples till a certain proportion ($0.2$) of the rest of the data points. We then assign these positive samples a label of +1 and all the negative samples a label of -1
\subsubsection{Parameter Selection}
The two main parameters for our dot product kernel are the learning rate and the regularizer. The smaller the learning rate, the larger the number of support vectors are added, and the more representative the overall function is. The regularizer term effectively controls the shrinking of the $\alpha$s. The parameters were obtained by running cross validation over different orders of magnitude for $\eta$ and $\lambda$ as shown in table \ref{cross_svm}

\subsubsection{Performance}
Please see fig \ref{svm_fig} and table \ref{svm_tab}. Typical run time for noise free is 
Training duration =  8.5953s
Testing duration =  4.1104s
\begin{figure}[h]
\centering
\includegraphics[width=0.4\linewidth]{figs/kernel_3D_amtoan.png}
\includegraphics[width=0.4\linewidth]{figs/kernel_am_to_an.png}
\caption{Kernel SVM trained on the am dataset, tested on the an dataset. The confusion matrix has been scaled column-wise to enhance visibility.}
\vspace{10pt}
\includegraphics[width=0.4\linewidth, trim = 0 0 0 35, clip]{figs/kernel_3D_antoam.png}
\includegraphics[width=0.4\linewidth]{figs/kernel_an_to_am.png}
\caption{Kernel SVM trained on the an dataset, tested on the am dataset. }
\label{svm_fig}
\end{figure}

\begin{table}
\centering
\begin{tabular}{||c|c||c|c|c|c|c|c||}
\hline
$\eta$&$\lambda$&Veg & Wire & Pole & Ground & Facade & Accuracy\\ \hline
0.004 & 0.01 &  0.48687824 &  0.06393718 &  0.02846637 &  0.16646118 & 0.22856888 & 0.63279377\\ \hline
0.01 & 0.01 & 0.36033327 & 0.23859458 & 0.12689121 & 0.07070051 & 0.20230931 &  0.59520834\\ \hline
0.1 & 0.01 & 0.11400762 & 0.19878134 & 0.08592745 & 0.44138744 & 0.19123841 & 0.50478872\\ \hline
0.0001 & 0.01 &  0.6059946 &  0.    &      0.46552533 & 0.01653522 & 0.05532819 & 0.62880983 \\ \hline
0.004 & 0.1 & 0.55922353 & 0.  &        0.05335758 & 0.04923961 & 0.15238126 & 0.54600107\\ \hline
0.004 & 10 &  0.43752623 & 0.  &        0.0882957 &  0.65539162 & 0. & 0.62219963  \\ \hline
\end{tabular}
\caption{F1 scores for cross validation tests on linear SVM $\eta$ and $\lambda$}
\label{cross_svm}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{||c|c|c||c|c|c|c|c||c||}
\hline
Comment&$\eta$&$\lambda$&Veg & Wire & Pole & Ground & Facade & Accuracy\\ \hline
No Noise&0.004 & 0.01 &0.77269466  & 0.         &  0.36314363  & 0.98022422   &0.63411819  & 0.7913\\ \hline
+100 Random&0.004 & 0.01 &  0.7934477 &  0.     &     0.         & 0.94660439 & 0. &  0.7669\\ \hline
$0.1 \sigma$&0.004 & 0.01 & 0.79392206  & 0.     &      0.3847298   & 0.97496277  & 0.60604134 & 0.8039\\ \hline
\end{tabular}
\caption{F1 score for Kernel SVM trained on am and tested on an}
\vspace{10pt}
\begin{tabular}{||c|c|c||c|c|c|c|c||c||}
\hline
Comment&$\eta$&$\lambda$&Veg & Wire & Pole & Ground & Facade & Accuracy\\ \hline
No Noise&0.004 & 0.01 & 0.48858833   &0.00225606  &0.6068527   & 0.99702255 &  0.716908  & 0.9015 \\ \hline
+100 Random&0.004 & 0.01 & 0.00240297 &  0.08423236  & 0.00833912 &  0.9772284&
   0.233722 &0.7779\\ \hline
$0.1 \sigma$&0.004 & 0.01 &  0.51048577  & 0.      &     0.66762756  & 0.99718548  & 0.5728698  &0.8836\\ \hline
\end{tabular}
\caption{F1 score for Kernel SVM trained on an and tested on am}
\label{svm_tab}
\end{table}
%-----------------------------------------------------------------------
\section{Comparative Analysis of Learners}
Surprisingly, the Bayes Linear Regression approach worked the best, perhaps because of the application of margin scaling. It seems that margin scaling is a more effective method of compensating for unequal distribution of training data than proportional data duplication. All methods seem to be affected very strongly by the assymetric data distribution.

\section{Future Work}
Future work can involve trying alternate techniques to mitigate the dataset bias problems, such as slack scaling or different loss functions. We could also augment features using features that encode spatial context, e.g., 3D HoG or sparsity in some way to better disambiguate the pole and wire classes. Judging by it's performance, I would prefer using some variant of regression on a real world robot.
\end{document}